{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5817d14-82ab-494f-8c5e-2c6cc66f738b",
   "metadata": {},
   "source": [
    "### TITLE : Sentiment Analysis of Twitter Feedback Data on Apple and Google Products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7061505b-8ea4-44a1-9b84-54ee5c68a5ab",
   "metadata": {},
   "source": [
    "start with traditional machine learnig models \n",
    "Traditional Machine Learning Models:\n",
    "\n",
    "#### Naive Bayes: Simple and efficient, works well with smaller datasets. Easy to interpret but might not capture complex sentiment nuances.\n",
    "#### Logistic Regression: Another good option for smaller datasets, easy to implement and interpret. Can be limited in handling intricate relationships within the data.\n",
    "#### Support Vector Machines (SVM): Effective for high-dimensional data, good at handling noise and outliers. Requires careful parameter tuning and can be computationally expensive.\n",
    "\n",
    "Deep Learning Models:\n",
    "\n",
    "Long Short-Term Memory (LSTM): Captures long-range dependencies in text, well-suited for analyzing sequential data like tweets. More complex and computationally expensive compared to traditional models.\n",
    "\n",
    "Convolutional Neural Networks (CNNs): Can learn patterns from local features in the text, effective for handling short texts like tweets. May require large datasets for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f1bf4-802f-406e-ad77-a84d1b533a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class for modeling\n",
    "# Define my class\n",
    "class Modelbuilder:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def preprocessing(self, X_train, X_test):\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        return X_train_scaled, X_test_scaled\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model.fit(X_train, y_train)    \n",
    "\n",
    "    def predict_train(self, X_train):\n",
    "        y_train_pred = self.model.predict(X_train)\n",
    "        return y_train_pred\n",
    "    \n",
    "    def predict_test(self, X_test):\n",
    "        y_test_pred = self.model.predict(X_test)\n",
    "        return y_test_pred\n",
    "    \n",
    "    def evaluate(self, y_train, y_train_pred, y_test, y_test_pred):\n",
    "        accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "        \n",
    "        accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "        f1_test = f1_score(y_test, y_test_pred, average='weighted')\n",
    "        class_report_test = clf_report(y_test, y_test_pred)\n",
    "        matrix_test = confusion_matrix(y_test, y_test_pred)\n",
    "        \n",
    "        evaluation_results = {\n",
    "            'train': {\n",
    "                'accuracy': accuracy_train\n",
    "            },\n",
    "            'test': {\n",
    "                'accuracy': accuracy_test,\n",
    "                'f1_score': f1_test,\n",
    "                'classification_report': class_report_test,\n",
    "                'confusion_matrix': matrix_test\n",
    "            }\n",
    "        }\n",
    "        return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e29fe4-e8a7-4f73-9250-331a2b8ac457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Instantiate Modelbuilder with your model\n",
    "model_builder_log = Modelbuilder(model)\n",
    "\n",
    "# Fit the model\n",
    "model_builder_log.fit(X_train_full, y_train)\n",
    "\n",
    "# Predict on the training set\n",
    "y_train_pred = model_builder_log.predict_train(X_train_full)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = model_builder_log.predict_test(X_test_full)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = model_builder_log.evaluate(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "# Print evaluation metrics for training set\n",
    "train_accuracy = evaluation_results['train']['accuracy']\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "# Print evaluation metrics for testing set\n",
    "test_results = evaluation_results['test']\n",
    "print(\"\\nTesting Accuracy:\", test_results['accuracy'])\n",
    "print(\"Testing F1 Score:\", test_results['f1_score'])\n",
    "print(\"Testing Classification Report:\\n\", test_results['classification_report'])\n",
    "\n",
    "# Get the unique class labels for testing set\n",
    "target_names = pd.Series(y_test_pred).unique().tolist()\n",
    "\n",
    "# Create a confusion matrix dataframe\n",
    "cm_df = pd.DataFrame(test_results['confusion_matrix'], index=target_names, columns=target_names)\n",
    "\n",
    "# Display confusion matrix as heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58329e6-b828-4ae6-a591-3855d5735fc0",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae751a70-c5c5-48ed-8843-7a8fde6a929e",
   "metadata": {},
   "source": [
    "### Baseline model\n",
    "#### 1. Naive Bayes\n",
    "WE chose the model because it is  simple and efficient to use.\n",
    "###### just need to know how to insantiate the models ..then the class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55c92c4-7937-4c4c-b447-01fb13ab9e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a MultinomialNB classifier with default params\n",
    "baseline_nb = MultinomialNB()\n",
    "...use above example on logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe7e2be-9b5f-4ab4-9d26-3bfb19105474",
   "metadata": {},
   "source": [
    "#### 2. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c96288-a602-4bc9-a08d-0747db23bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an instance of the LogisticRegression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "......use above example on logistic regression\n",
    "\n",
    "# Optionally, you can specify hyperparameters during model creation\n",
    "#model = LogisticRegression(solver='liblinear', random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0359dae-35b1-4ef6-87fe-1dd0dd4598e2",
   "metadata": {},
   "source": [
    "#### 3. decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b995d64-e0ee-4b1b-a405-2b9f681dcb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate class\n",
    "baseline_dt = DecisionTreeClassifier()\n",
    "......use above example on logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003bb6e-99d5-46fc-8bc1-e5ce8c7a03be",
   "metadata": {},
   "source": [
    "#### 4.Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298b2e7-7720-412f-bd1c-a64ec4e12db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate support vector classifier\n",
    "svc = SVC()\n",
    "\n",
    ".....use above example on logistic regression\n",
    "\n",
    "# Initialize StandardScaler with with_mean=False\n",
    "#scaler = StandardScaler(with_mean=False)\n",
    "\n",
    "# Fit and transform the training data\n",
    "#X_train_final_regex_resampled_scaled = scaler.fit_transform(X_train_final_regex_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f03af-e33c-4ee1-8186-a67746ebedd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7e3fa61-c6fd-4776-812d-bb90b2f72f5a",
   "metadata": {},
   "source": [
    "#### DEEP LEARNING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3189b9db-e4c1-4e6d-85af-c2c61721b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the tweet column to a DataFrame\n",
    "main_df[\"tweet_df\"] = main_df[\"clean_text\"].apply(lambda tokens: ' '.join(map(str, tokens)))\n",
    "\n",
    "# creating a vocabulary of unique tokens\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(main_df[\"tweet_df\"])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# converting text to integer sequences\n",
    "sequences = tokenizer.texts_to_sequences(main_df[\"tweet_df\"])# padding sequences to the fixed length\n",
    "\n",
    "# padding sequences to the fixed length\n",
    "padded_sequences = pad_sequences(sequences, maxlen = 100, padding = \"post\", truncating = \"post\")\n",
    "\n",
    "# assigning the padded sequences back to the DataFrame\n",
    "main_df[\"tweet_df\"] = list(padded_sequences)\n",
    "\n",
    "# displaying the unique values\n",
    "unique_values = main_df[\"tweet_df\"].apply(tuple).unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dcfe96-86d1-4138-aa5c-f9645f30af75",
   "metadata": {},
   "source": [
    "#### 1. LSTM MODEL\n",
    "Long Short-Term Memory, are a type of recurrent neural network (RNN) architecture specifically designed to address a major limitation of traditional RNNs: the vanishing gradient problem.\n",
    "\n",
    "Here's why I selected this model:(Try to paraphrase this)\n",
    "\n",
    "1. Sequential Data Handling: LSTM networks are well-suited for handling sequences of data, making them ideal for analyzing text data like tweets. They can capture dependencies and patterns in the text over time, which is crucial for understanding sentiment in sentences and paragraphs.\n",
    "\n",
    "2. Avoiding Vanishing Gradient: LSTMs are designed to mitigate the vanishing gradient problem that affects traditional recurrent neural networks (RNNs). This property allows LSTMs to capture long-range dependencies in text, ensuring that the model doesn't overlook crucial information.\n",
    "\n",
    "3. Embedding Layer: The model starts with an embedding layer, which helps convert words into numerical representations. This layer allows the model to learn meaningful representations of words, which is vital for understanding the semantics of the text.\n",
    "\n",
    "4. Deep Learning Capability: By adding multiple layers (in your case, LSTM, Dense, and Output layers), the model can learn complex relationships within the data. Deep learning is often beneficial when dealing with nuanced sentiment expressions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09142eb4-4eb6-49a1-8f76-c8bff0d238cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X variable \n",
    "X = main_df[\"tweet_df\"]\n",
    "\n",
    "# target variable\n",
    "y = main_df[\"sentiment\"]\n",
    "\n",
    "# carry out train test split 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f332265-8f3c-4d38-81c1-54e4ce03b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the data types\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# ensuring that target labels are encoded for the classes to represent the emotions\n",
    "y_train_encoded = to_categorical(y_train, num_classes = 2)\n",
    "y_test_encoded = to_categorical(y_test, num_classes = 2)\n",
    "\n",
    "# converting into to an array\n",
    "y_train_encoded = np.array(y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000e66cb-0e15-4281-83fd-59e886ecd884",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen = 100)\n",
    "X_test = pad_sequences(X_test, maxlen = 100)\n",
    "num_classes = 2  \n",
    "y_train_encoded = to_categorical(y_train, num_classes = 2)\n",
    "y_test_encoded = to_categorical(y_test, num_classes = 2)\n",
    "\n",
    "# instantianting the model \n",
    "model = Sequential()\n",
    "\n",
    "# constructing a neural network in embedding layer\n",
    "model.add(Embedding(input_dim = vocab_size, output_dim=64, input_length=100))  \n",
    "\n",
    "# gru layer, dense layer, output layer\n",
    "model.add(LSTM(128, activation = \"relu\"))\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(Dense(2, activation = \"softmax\"))  \n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "\n",
    "# training the model\n",
    "history = model.fit(X_train, y_train_encoded, epochs = 10, batch_size = 64, validation_data = (X_test, y_test_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3f7857-e565-429d-9a7a-04eed36b9761",
   "metadata": {},
   "source": [
    "#### 2. RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b759078-bf84-450a-a820-b982ac2f0b37",
   "metadata": {},
   "source": [
    "Went ahead to see if  a deep learning model based on the RNN architecture would perform better compared to the traditional LSTM  model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289ee4d-e088-491a-a95e-e2aa13541c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Define the model RNN \n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(input_dim=vocab_size, output_dim=64, input_length=100))\n",
    "model_rnn.add(SpatialDropout1D(0.2))\n",
    "model_rnn.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_rnn.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_rnn = model_rnn.fit(X_train, y_train_encoded, epochs=10, batch_size=64, validation_data=(X_test, y_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d8b130-f8f8-4210-bf17-c3f166bd8566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1db686c-f490-4da5-bd1d-5ac5f724eb53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5a675-c1e9-4064-8259-e7b72593bd2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2befa897-6dbd-4fa3-bbbe-33dcd83e343a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92485470-4dc7-4805-a263-10d976463a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc4473d-da97-4cc2-9cfa-e69e4cb27a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b3f2f-f35c-4be1-8abb-b07c24fc6b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec15456a-0c1f-4912-9256-08b5005737cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1427131c-5557-44af-92f2-125c9b16b37c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f516d-1e0c-435f-82f3-57f76b0390f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2.RNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2365336-5e7a-4e73-a4ea-2a89dd05bdf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
